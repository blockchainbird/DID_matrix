# DID_matrix

Personal ranking tool DID methods [registered at W3C](https://w3c-ccg.github.io/did-method-registry/)

## Philosophy

Why are we shedding a light on the quality and status of distinctive DID methods
* an essential topic for personal freedom
* real developments are not transparent
* it is a complex and extensive field

How could the tool be used and what are the expected results?
* Filter and select DID methods for your organisational or personal use
* Choose based on objective criteria, scores and personal weights related to your use case
* Quick fix to skip lengthy assessments

# How it works
A list of objective criteria has been structured into a spreadsheet. The spreadsheet offers the ability to rank projects (methods) in the DID registry. 
Based on objective `scores` (compy or explain) and subjective `weights` (opinion).

# Main categories
1. github / bitbucket activity: code, people, issues, wiki, forks, branches. (numbers and date)
2. (de)centralisation: any intermediation? (steering groups, ownership, investors, patents, licenses)
3. Recent communication (number and date of interactions in social media about the method)
4. Timeline and roadmap (any present? sticked to ….?)
5. Quality of the method description: Completeness, Interoperability, creativity, uniqueness
6. Ease of use of the method (operational, webservice for testing, sandbox, examples, blockchains / DLTs to create the DIDs
7. another one

# Default scores and timestamps
We'll provide default scores for registered methods against these criteria and use a spreadsheet to calculate marks. We will add the timestamp of our assessment and explain the score. Any users could alter these scores and weights.

# Help us improve the Ranking tool DID_matrix
Did we miss out on important criteria? Do you think that we should skip one or more criteria that we’ve listed?

**Feel free to contribute**
